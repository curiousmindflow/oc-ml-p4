{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"overview\": {\n",
    "\n",
    "    },\n",
    "    \"fe\": {\n",
    "        \"id\": False, # true\n",
    "        \"volume\": False,\n",
    "        \"order_purchase_timestamp\": False, # linked with 'shipping_limit_date'\n",
    "        \"shipping_limit_date\": False,\n",
    "        \"order_estimated_delivery_date\": False,\n",
    "        \"price_agg\": False, # false\n",
    "        \"last_order_datetime\": False,\n",
    "        \"frequency\": False\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"rfm_raw\": {\n",
    "            \"preparation\": False,\n",
    "            \"n_cluster\": False,\n",
    "            \"silhouette\": False,\n",
    "            \"explain\": False,\n",
    "            \"persona\": False\n",
    "        },\n",
    "\n",
    "    },\n",
    "    \"pca\": {\n",
    "        \"do\": True,\n",
    "        \"elbow\": True,\n",
    "        \"silhouette\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1 Dependencies import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer, KElbowVisualizer\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1.1 Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_features_name(dataset, split_by_unique_count=True, split_count=10):\n",
    "    features_name = dataset.select_dtypes([\"object\", \"bool\"]).columns\n",
    "    if split_by_unique_count:\n",
    "        less_uniques = [feature_name for feature_name in features_name if dataset[feature_name].nunique() <= split_count]\n",
    "        lot_uniques = features_name.difference(less_uniques).tolist()\n",
    "        return (less_uniques, lot_uniques)\n",
    "    else:\n",
    "        return features_name.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_features_name(dataset):\n",
    "    features_name = dataset.select_dtypes([\"int64\", \"float64\"]).columns.values.tolist()\n",
    "    return features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressiveFeatureSelection(df, n_clusters=3, max_features=4,):\n",
    "    feature_list = list(df.columns)\n",
    "    selected_features = list()\n",
    "    # select starting feature\n",
    "    initial_feature = \"\"\n",
    "    high_score = 0\n",
    "    for feature in feature_list:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        data_ = df[feature]\n",
    "        labels = kmeans.fit_predict(data_.to_frame())\n",
    "        score_ = silhouette_score(data_.to_frame(), labels)\n",
    "        print(\"Proposed new feature {} with score {}\". format(feature, score_))\n",
    "        if score_ >= high_score:\n",
    "            initial_feature = feature\n",
    "            high_score = score_\n",
    "    print(\"The initial feature is {} with a silhouette score of {}.\".format(initial_feature, high_score))\n",
    "    feature_list.remove(initial_feature)\n",
    "    selected_features.append(initial_feature)\n",
    "    for _ in range(max_features-1):\n",
    "        high_score = 0\n",
    "        selected_feature = \"\"\n",
    "        print(\"Starting selection {}...\".format(_))\n",
    "        for feature in feature_list:\n",
    "            selection_ = selected_features.copy()\n",
    "            selection_.append(feature)\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            data_ = df[selection_]\n",
    "            labels = kmeans.fit_predict(data_)\n",
    "            score_ = silhouette_score(data_, labels)\n",
    "            print(\"Proposed new feature {} with score {}\". format(feature, score_))\n",
    "            if score_ > high_score:\n",
    "                selected_feature = feature\n",
    "                high_score = score_\n",
    "        selected_features.append(selected_feature)\n",
    "        feature_list.remove(selected_feature)\n",
    "        print(\"Selected new feature {} with score {}\". format(selected_feature, high_score))\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1.2 Pipeline construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, model, scoring=\"neg_root_mean_squared_error\"):\n",
    "\n",
    "    ### DATASET PREPARATION ###\n",
    "\n",
    "    # categorical_cols = get_categorical_features_name(dataset, split_by_unique_count=False)\n",
    "    numerical_cols = get_numerical_features_name(dataset)\n",
    "\n",
    "    # X = dataset[numerical_cols + categorical_cols]\n",
    "    X = dataset[numerical_cols]\n",
    "\n",
    "    ### PIPELINE CONSTRUCTION ###\n",
    "\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"simple_imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"minmax_scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # cat_pipe = Pipeline(steps=[\n",
    "    #     (\"simple_imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    #     (\"ordinal_encoder\", OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)),\n",
    "    #     (\"simple_imputer_bis\", SimpleImputer(strategy=\"mean\")),\n",
    "    #     (\"std_scaler\", StandardScaler())\n",
    "    # ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num_pipe\", num_pipe, numerical_cols),\n",
    "        # (\"cat_pipe\", cat_pipe, categorical_cols)\n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"transforms\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    preprocessor.fit(X)\n",
    "    array_preproc = preprocessor.transform(X)\n",
    "    data_preproc = pd.DataFrame(data=array_preproc, columns=X.columns)\n",
    "\n",
    "    return pipeline, preprocessor, data_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_preprocessor(dataset):\n",
    "    num_cols = get_numerical_features_name(dataset)\n",
    "    categ_cols, _ = get_categorical_features_name(dataset)\n",
    "\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imputer_01\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler_01\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categ_pipe = Pipeline(steps=[\n",
    "        (\"imputer_01\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder_01\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num_pipe\", num_pipe, num_cols),\n",
    "        (\"categ_pipe\", categ_pipe, categ_cols)\n",
    "    ])\n",
    "\n",
    "    return preprocessor, num_cols, categ_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    preprocessor, num_cols, categ_cols = pca_preprocessor(dataset)\n",
    "\n",
    "    raw_preprocessed_data = preprocessor.fit_transform(dataset)\n",
    "\n",
    "    if categ_cols:\n",
    "        categ_cols_preprocessed = preprocessor.transformers_[1][1][\"encoder_01\"].get_feature_names_out(categ_cols).tolist()\n",
    "    else:\n",
    "        categ_cols_preprocessed = []\n",
    "\n",
    "    col_names = num_cols + categ_cols_preprocessed\n",
    "\n",
    "    preprocessed_data = pd.DataFrame(data=raw_preprocessed_data, columns=col_names)\n",
    "\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1.3 Cluster evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_plot(range, data, figsize=(10,10)):\n",
    "    intertia_list = []\n",
    "    for n in range:\n",
    "        kmeans = KMeans(n_clusters=n, random_state=1)\n",
    "        kmeans.fit(data)\n",
    "        intertia_list.append(kmeans.inertia_)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.lineplot(y=intertia_list, x=range, ax=ax)\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Inertia\")\n",
    "    ax.set_xticks(list(range))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_plot(range, data, n_cols=2, figsize=(20, 10), wspace=0.35, hspace=0.35):\n",
    "    if range[0] == 0:\n",
    "        raise Exception(\"Range must not include 0\")\n",
    "\n",
    "    n_rows = ceil(len(range)/n_cols)\n",
    "    index = 0\n",
    "\n",
    "    plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    plt.subplots_adjust(wspace=wspace, hspace=hspace)\n",
    "\n",
    "    for n in range:\n",
    "        kmeans = KMeans(n, random_state=1)\n",
    "\n",
    "        plt.subplot(n_rows, n_cols, index+1)\n",
    "        sv = SilhouetteVisualizer(kmeans, colors=\"yellowbrick\")\n",
    "        sv.fit(data)\n",
    "\n",
    "        index += 1\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_epsilon(n_neighbors, data):\n",
    "    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nbrs = neigh.fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    plt.plot(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1.4 Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scree_plot(dataset, figsize=(15, 5)):\n",
    "    pca = PCA()\n",
    "    pca.fit(dataset)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    explain_variance = pd.Series(pca.explained_variance_ratio_)\n",
    "    explain_variance.plot(kind=\"bar\", alpha=0.7)\n",
    "\n",
    "    total = 0\n",
    "    var_ls = []\n",
    "    for x in explain_variance:\n",
    "        total = total + x\n",
    "        var_ls.append(total)\n",
    "    \n",
    "    pd.Series(var_ls).plot(marker=\"o\", alpha=0.7)\n",
    "    plt.xlabel(\"Principle Components\", fontsize=\"x-large\")\n",
    "    plt.ylabel(\"Percentage Variance Explained\", fontsize=\"x-large\")\n",
    "    plt.title(\"Scree plot\", fontsize=\"xx-large\")\n",
    "    plt.show()\n",
    "\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(dataset, columns, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_data = pca.fit_transform(dataset[columns])\n",
    "    components_name = [f\"PC{i+1}\" for i in range(pca_data.shape[1])]\n",
    "    pca_data = pd.DataFrame(data=pca_data, columns=components_name)\n",
    "    loadings = pd.DataFrame(data=pca.components_.T, columns=components_name, index=columns)\n",
    "    return pca, pca_data, loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    evr = pca.explained_variance_ratio_\n",
    "\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "\n",
    "    cumulative_variance = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cumulative_variance], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1.5 RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfm_distplot(dataset, customer_id, figsize=(20, 5)):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    dataset = dataset.loc[:, dataset.columns.difference(customer_id)]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    for i, feature in enumerate(dataset.columns):\n",
    "        sns.distplot(dataset[feature], ax=axes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(dataset, figsize=(30, 20)):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    correlation = dataset.corr()\n",
    "    mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "\n",
    "    sns.heatmap(data=correlation, mask=mask, annot=True, vmax=.75, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    plt.title(\"Correlation heatmap\", size=20)\n",
    "    plt.xticks(rotation=45, size=16, ha=\"right\")\n",
    "    plt.yticks(size=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow(dataset, k=(2, 12)):\n",
    "    model = KMeans(random_state=1)\n",
    "    elbow_visualizer = KElbowVisualizer(model, k=k)\n",
    "\n",
    "    elbow_visualizer.fit(dataset)\n",
    "    elbow_visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette(dataset, n):\n",
    "    model = KMeans(n, random_state=1)\n",
    "    silhouette_visualizer = SilhouetteVisualizer(model)\n",
    "\n",
    "    silhouette_visualizer.fit(dataset)\n",
    "    silhouette_visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(dataset, dataset_preproc):\n",
    "    model = KMeans(n_clusters=4, random_state=1).fit(dataset_preproc)\n",
    "    data_explain = dataset\n",
    "    data_explain[\"Cluster\"] = model.labels_\n",
    "\n",
    "    data_explain_melt = pd.melt(data_explain, id_vars=[\"customer_id\", \"Cluster\"], value_vars=[\"Récence\", \"Fréquence\", \"Montant\"], var_name=\"Features\", value_name=\"Value\")\n",
    "    sns.lineplot(\"Features\", \"Value\", hue=\"Cluster\", data=data_explain_melt)\n",
    "    plt.legend()\n",
    "\n",
    "    return data_explain.groupby(\"Cluster\").agg({\n",
    "    \"Récence\": [\"mean\", \"min\", \"max\"],\n",
    "    \"Fréquence\": [\"mean\", \"min\", \"max\"],\n",
    "    \"Montant\": [\"mean\", \"min\", \"max\", \"count\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1.6 Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_global(dataset, dataset_preproc, clusters, figsize=(30, 10)):\n",
    "    model = KMeans(n_clusters=clusters, random_state=1).fit(dataset_preproc)\n",
    "    data_explain = dataset.copy()\n",
    "    data_explain.loc[:, \"Cluster\"] = model.labels_\n",
    "\n",
    "    data_explain_melt = pd.melt(data_explain, id_vars=[\"customer_id\", \"Cluster\"], value_vars=dataset_preproc.columns, var_name=\"Features\", value_name=\"Value\")\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.lineplot(x=\"Features\", y=\"Value\", hue=\"Cluster\", data=data_explain_melt)\n",
    "    plt.legend()\n",
    "\n",
    "    return data_explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data_cleaned.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data/data_cleaned.csv\", delimiter=\",\", parse_dates=[\"order_purchase_timestamp\", \"order_approved_at\", \"order_delivered_carrier_date\", \"order_delivered_customer_date\", \"order_estimated_delivery_date\", \"review_creation_date\", \"review_answer_timestamp\", \"shipping_limit_date\"], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3 Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 4 Feeture engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Remove *_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_id_remove(dataset, diff=[]):\n",
    "    id_cols = dataset.columns[dataset.columns.str.contains(\"_id\")]\n",
    "    id_cols = id_cols.difference(diff)\n",
    "    dataset.drop(columns=id_cols, inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.1 Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_volume(dataset):\n",
    "    dataset[\"product_volume_cm3\"] = dataset[\"product_length_cm\"] * dataset[\"product_height_cm\"] * dataset[\"product_width_cm\"]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.2 Order purchase timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_purchase_timestamp(dataset):\n",
    "    dataset.loc[:, \"order_purchase_timestamp\"] = dataset.loc[:, \"order_purchase_timestamp\"].apply(pd.to_datetime)\n",
    "    opt = dataset.loc[:, \"order_purchase_timestamp\"]\n",
    "    dataset[\"order_purchase_year\"] = opt.dt.year\n",
    "    dataset[\"order_purchase_month\"] = opt.dt.month\n",
    "    dataset[\"order_purchase_day\"] = opt.dt.day\n",
    "    dataset[\"order_purchase_hour\"] = opt.dt.hour\n",
    "\n",
    "    display = dataset.loc[:, [\"order_purchase_timestamp\", \"order_purchase_year\", \"order_purchase_month\", \"order_purchase_day\", \"order_purchase_hour\"]].head()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.3 Shipping limit date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_shipping_date(dataset):\n",
    "    dataset.loc[:, \"shipping_limit_date\"] = dataset.loc[:, \"shipping_limit_date\"].apply(pd.to_datetime)\n",
    "    sld = data.loc[:, \"shipping_limit_date\"]\n",
    "    dataset[\"shipping_limit_year\"] = sld.dt.year\n",
    "    dataset[\"shipping_limit_month\"] = sld.dt.month\n",
    "    dataset[\"shipping_limit_day\"] = sld.dt.day\n",
    "    dataset[\"shipping_limit_hour\"] = sld.dt.hour\n",
    "\n",
    "    display = dataset.loc[:, [\"shipping_limit_date\", \"shipping_limit_year\", \"shipping_limit_month\", \"shipping_limit_day\", \"shipping_limit_hour\"]].head()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.4 Estimated delivery date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_delivery_date(dataset):\n",
    "    dataset.loc[:, \"order_estimated_delivery_date\"] = dataset.loc[:, \"order_estimated_delivery_date\"].apply(pd.to_datetime)\n",
    "    oedd = dataset.loc[:, \"order_estimated_delivery_date\"]\n",
    "    dataset[\"order_estimated_delivery_year\"] = oedd.dt.year\n",
    "    dataset[\"order_estimated_delivery_month\"] = oedd.dt.month\n",
    "    dataset[\"order_estimated_delivery_day\"] = oedd.dt.day\n",
    "    dataset[\"order_estimated_delivery_hour\"] = oedd.dt.hour\n",
    "\n",
    "    display = dataset.loc[:, [\"order_estimated_delivery_date\", \"order_estimated_delivery_year\", \"order_estimated_delivery_month\", \"order_estimated_delivery_day\", \"order_estimated_delivery_hour\"]].head()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.5 Price agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_price(dataset):\n",
    "    customer_price_agg = dataset.groupby(\"customer_id\").agg({\n",
    "        \"price\": [\"min\", \"max\", \"mean\", \"sum\"]\n",
    "    })\n",
    "\n",
    "    customer_price_agg.rename(columns={\n",
    "        \"min\": \"price_min\",\n",
    "        \"max\": \"price_max\",\n",
    "        \"mean\": \"price_mean\",\n",
    "        \"sum\": \"price_sum\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    dataset = dataset.merge(customer_price_agg.price, on=\"customer_id\")\n",
    "    display = dataset.loc[:, [\"customer_id\", \"price_min\", \"price_max\", \"price_mean\", \"price_sum\"]].head()\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.6 Last order datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_recency(dataset):\n",
    "    dataset.loc[:, \"order_purchase_timestamp\"] = dataset.loc[:, \"order_purchase_timestamp\"].apply(pd.to_datetime)\n",
    "    data_end = max(dataset.loc[:, \"order_purchase_timestamp\"] + pd.Timedelta(days=1))\n",
    "\n",
    "    opt_agg = dataset.groupby(\"customer_id\").agg({\n",
    "        \"order_purchase_timestamp\": lambda x: (data_end - max(x)).days\n",
    "    })\n",
    "\n",
    "    opt_agg.columns = [\"recency\"]\n",
    "    opt_agg = opt_agg.reset_index()\n",
    "\n",
    "    dataset = dataset.merge(opt_agg, on=\"customer_id\")\n",
    "\n",
    "    display = dataset.loc[:, [\"customer_id\", \"order_purchase_timestamp\", \"recency\"]].head()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.7 Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_frequency(dataset):\n",
    "    frequency = dataset.groupby(\"customer_id\").agg({\n",
    "        \"order_id\": [\"count\"]\n",
    "    })\n",
    "\n",
    "    frequency = frequency.order_id.reset_index()\n",
    "    frequency.columns = [\"customer_id\", \"frequency\"]\n",
    "    \n",
    "    dataset = dataset.merge(frequency, how=\"left\", on=\"customer_id\")\n",
    "    display = dataset.loc[:, [\"customer_id\", \"frequency\"]].head()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 5 Modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 RFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFM: Récence (dat de la dernière commande), Fréquence (des commandes), Montant (de la dernière commande ou sur une période donnée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"preparation\"]:\n",
    "    rfm_cols = [\"customer_id\",\"order_purchase_timestamp\", \"order_id\", \"price\"]\n",
    "    data_rfm = data.loc[:, rfm_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"preparation\"]:\n",
    "    data_rfm = fe_id_remove(data_rfm, diff=[\"customer_id\", \"order_id\", \"product_id\"])\n",
    "    data_rfm = fe_recency(data_rfm)\n",
    "    data_rfm = fe_price(data_rfm)\n",
    "    data_rfm = fe_frequency(data_rfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"preparation\"]:\n",
    "    data_rfm.drop(columns=[\"price\", \"price_min\", \"price_max\", \"price_mean\", \"order_purchase_timestamp\", \"order_id\"], inplace=True)\n",
    "    data_rfm.rename(columns={\"recency\": \"Récence\", \"price_sum\": \"Montant\", \"frequency\": \"Fréquence\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "if config[\"model\"][\"rfm_raw\"][\"preparation\"]:\n",
    "    display = data_rfm.describe()\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"preparation\"]:\n",
    "    rfm_distplot(data_rfm, [\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"preparation\"]:\n",
    "    correlation_heatmap(data_rfm, figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"preparation\"]:\n",
    "\n",
    "    model = KMeans(n_clusters=4)\n",
    "\n",
    "    pipeline, preproc, data_preproc = evaluate(data_rfm, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "if config[\"model\"][\"rfm_raw\"][\"preparation\"]:\n",
    "    display = data_preproc.head()\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"preparation\"]:\n",
    "    rfm_distplot(data_preproc, [\"customer_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 5.1.2 Cluster N analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"n_cluster\"]:\n",
    "    elbow(data_preproc.copy(), (2, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"silhouette\"]:\n",
    "    silhouette(data_preproc.copy(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette for n=4  \n",
    "![alt text](plots/kmeans_4n.png \"Silhouette for n=4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 5.1.3 Explaination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "if config[\"model\"][\"rfm_raw\"][\"explain\"]:\n",
    "    display = explain(data_rfm, data_preproc)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"explain\"]:\n",
    "    pred = KMeans(n_clusters=4, random_state=1).fit_predict(data_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"explain\"]:\n",
    "    plt.subplots(3, 3, figsize=(30, 15))\n",
    "    plt.subplots_adjust(wspace=0.15, hspace=0.15)\n",
    "\n",
    "    plt.subplot(3 ,3, 1)\n",
    "    sns.scatterplot(x=data_rfm[\"Récence\"], y=data_rfm[\"Récence\"], hue=pred, palette=\"deep\")\n",
    "\n",
    "    plt.subplot(3 ,3, 2)\n",
    "    sns.scatterplot(x=data_rfm[\"Récence\"], y=data_rfm[\"Fréquence\"], hue=pred, palette=\"deep\")\n",
    "\n",
    "    plt.subplot(3 ,3, 3)\n",
    "    sns.scatterplot(x=data_rfm[\"Récence\"], y=data_rfm[\"Montant\"], hue=pred, palette=\"deep\")\n",
    "\n",
    "    plt.subplot(3 ,3, 4)\n",
    "    sns.scatterplot(x=data_rfm[\"Fréquence\"], y=data_rfm[\"Récence\"], hue=pred, palette=\"deep\")\n",
    "\n",
    "    plt.subplot(3 ,3, 5)\n",
    "    sns.scatterplot(x=data_rfm[\"Fréquence\"], y=data_rfm[\"Fréquence\"], hue=pred, palette=\"deep\")\n",
    "\n",
    "    plt.subplot(3 ,3, 6)\n",
    "    sns.scatterplot(x=data_rfm[\"Fréquence\"], y=data_rfm[\"Montant\"], hue=pred, palette=\"deep\")\n",
    "\n",
    "    plt.subplot(3 ,3, 7)\n",
    "    sns.scatterplot(x=data_rfm[\"Montant\"], y=data_rfm[\"Récence\"], hue=pred, palette=\"deep\")\n",
    "\n",
    "    plt.subplot(3 ,3, 8)\n",
    "    sns.scatterplot(x=data_rfm[\"Montant\"], y=data_rfm[\"Fréquence\"], hue=pred, palette=\"deep\")\n",
    "\n",
    "    plt.subplot(3 ,3, 9)\n",
    "    sns.scatterplot(x=data_rfm[\"Montant\"], y=data_rfm[\"Montant\"], hue=pred, palette=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"explain\"]:\n",
    "    pca, pca_data, loadings = apply_pca(data_preproc, data_preproc.columns, 2)\n",
    "    reduced_data = pca_data.to_numpy()\n",
    "    model = KMeans(init=\"k-means++\", n_clusters=4)\n",
    "    model.fit(reduced_data)\n",
    "\n",
    "    h = 0.02\n",
    "\n",
    "    x_min = reduced_data[:, 0].min() - 1\n",
    "    x_max = reduced_data[:, 0].max() + 1\n",
    "\n",
    "    y_min = reduced_data[:, 1].min() - 1\n",
    "    y_max = reduced_data[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(\n",
    "        Z, interpolation=\"nearest\",\n",
    "        extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "        cmap=plt.cm.Paired,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\"\n",
    "    )\n",
    "\n",
    "    plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "\n",
    "    centroids = model.cluster_centers_\n",
    "    plt.scatter(\n",
    "        centroids[:, 0],\n",
    "        centroids[:, 1],\n",
    "        marker=\"x\",\n",
    "        s=169,\n",
    "        linewidths=3,\n",
    "        color=\"w\",\n",
    "        zorder=10\n",
    "    )\n",
    "\n",
    "    plt.title(\"K-means clustering, PCA reduced dataset\")\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"explain\"]:\n",
    "    loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"explain\"]:\n",
    "    plot_variance(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 5.1.4 Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"persona\"]:\n",
    "    display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"persona\"]:\n",
    "    data_preproc[\"cluster\"] = pred\n",
    "    data_preproc[\"order_purchase_timestamp\"] = data[\"order_purchase_timestamp\"]\n",
    "    data_preproc = fe_purchase_timestamp(data_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"persona\"]:\n",
    "    data_preproc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"persona\"]:\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.kdeplot(data=data_preproc, x=\"order_purchase_month\", hue=\"cluster\", shade=False, palette=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"persona\"]:\n",
    "    data_preproc[\"product_category_name\"] = data[\"product_category_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"persona\"]:\n",
    "    data_preproc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"persona\"]:\n",
    "    tmp = data_preproc.groupby(\"cluster\").agg({\n",
    "        \"product_category_name\": \"value_counts\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"persona\"]:\n",
    "    tmp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"persona\"]:\n",
    "    tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model\"][\"rfm_raw\"][\"persona\"]:\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    sns.countplot(data=data_preproc, x=\"product_category_name\", hue=\"cluster\", palette=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 0 - Persona 1 - Anciens\n",
    "\n",
    "Dernier achat: il y a plus de 1 an  \n",
    "Nombre d'achats: 1.2  \n",
    "Montant: 122€  \n",
    "Population: 45 156 / 40.42%  \n",
    "\n",
    "Actions:  \n",
    "...\n",
    "\n",
    "\n",
    "#### Cluster 1 - Persona 2 - Fidèle\n",
    "\n",
    "Dernier achat: il y a 8 mois  \n",
    "Nombre d'achats: 5.4  \n",
    "Montant: 358€  \n",
    "Population: 4 659 / 4.17%  \n",
    "\n",
    "\n",
    "#### Cluster 2 - Persona 3 - Nouveaux\n",
    "\n",
    "Dernier achat: il y a 4 mois  \n",
    "Nombre d'achats: 1.2  \n",
    "Montant: 122€  \n",
    "Population: 60 284 / 53.96%  \n",
    "\n",
    "\n",
    "#### Cluster 3 - Persona 4 - Fort potentiel\n",
    "\n",
    "Dernier achat: il y a 8 mois  \n",
    "Nombre d'achats: 1.7  \n",
    "Montant: 819€  \n",
    "Population: 1 606 / 1.43%  \n",
    "\n",
    "Actions:  \n",
    "Client à fort potentiel. Mise en place d'une action de fidélisation nécessaire.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5.2 Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "\n",
    "if config[\"pca\"][\"do\"]:\n",
    "    global_cols = data.select_dtypes([\"int64\", \"float64\"]).columns\n",
    "    global_cols = global_cols.append(data.loc[:, [\"customer_id\"]].columns)\n",
    "\n",
    "    global_data = data[global_cols]\n",
    "\n",
    "    global_data = fe_price(global_data)\n",
    "    global_data = fe_volume(global_data)\n",
    "\n",
    "    global_data.drop(columns=[\"order_item_id\", \"review_score\", \"payment_sequential\", \"payment_installments\", \"freight_value\", \"product_name_lenght\", \"product_photos_qty\", \"product_length_cm\", \"product_height_cm\", \"product_width_cm\"], inplace=True)\n",
    "\n",
    "    global_data_preproc = preprocess_data(global_data)\n",
    "\n",
    "    display = global_data_preproc.head()\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"pca\"][\"do\"]:\n",
    "    pca = scree_plot(global_data_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"pca\"][\"elbow\"]:\n",
    "    elbow(global_data_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"pca\"][\"silhouette\"]:\n",
    "    silhouette(global_data_preproc, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"pca\"][\"do\"]:\n",
    "    explain_data = explain_global(global_data, global_data_preproc, 6)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "586ad1ed5c97141e2437e681efbf1ec0adcd17d830cf5af2ca3d2819e743e158"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
